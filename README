OpenEmbedded/Yocto BSP layer for NVIDIA Tegra X1/X2/AGX/K1
==========================================================

**WORK IN PROGRESS** - Please do not use this branch for
any production development.

Boards supported:
* Jetson-TK1 development kit (Linux4Tegra R21.7)
* Jetson-TX1 development kit (Linux4Tegra R32.2, JetPack 4.2.1
* Jetson-TX2 development kit (Linux4Tegra R32.2, JetPack 4.2.1)
* Jetson AGX Xavier development kit (Linux4Tegra R32.2, JetPack 4.2.1)
* Jetson Nano development kit (Linux4Tegra R32.2, JetPack 4.2.1)

This layer depends on:
URI: git://git.openembedded.org/openembedded-core
branch: master
LAYERSERIES_COMPAT: warrior

WORK-IN-PROGRESS STATUS
-----------------------

* core-image-sato builds, boots, and passes basic tests on
  TX1, TX2, Nano, Xavier

* core-image-weston builds and boots on TX1, TX2, Nano, Xavier,
  with some issues:
  - gstreamer nveglgles plugin not displaying anything
  - capture test not working on Jetson-TX1

* BUP packages are now getting built, still need testing

TO DO
-----

* look into the nveglgles gstreamer issue with Weston

* test OTA/BUP on TX2 and Xavier

* look at OTA/BUP for TX1/Nano, since it looks like
  there's some support there now

* look at building trusty and cboot (t194) from sources

* look at container support

* run through more tests and fix up any issues


PLEASE NOTE
-----------

* Starting with JetPack 4.2, packages outside the L4T BSP can
  only be downloaded with an NVIDIA Developer Network login.
  So to use CUDA 10, cuDNN, and any other packages that require
  a Devnet login, you **must** create a Devnet account and
  download the JetPack packages you need for your builds using
  NVIDIA SDK Manager.

  You must then set the variable NVIDIA_DEVNET_MIRROR to
  "file://path/to/the/downloads" in your build configuration
  (e.g., local.conf) to make them available to your bitbake
  builds.

* The tensorrt 5.1.6 packages for Xavier, are different from
  those for TX1/TX2, even though the deb files have the same
  name.  If you need to build for Xavier and another platform
  and include tensorrt 5.1.6, create a subdirectory called
  "P2888" under your NVIDIA_DEVNET_MIRROR directory, and copy
  the Xavier tensorrt packages there. The non-Xavier copies
  should go in the NVIDIA_DEVNET_MIRROR top level.

* CUDA 10 supports up through gcc 7 only, and some NVIDIA-provided
  binary libraries appear to be compiled with g++ 7 and cause linker
  failures when building applications with g++ 6, so **only** gcc 7
  should be used if you intend to use CUDA. (For Jetson-TK1, CUDA 6.5
  supports up through gcc 5.x only.)


Selecting the toolchain version
-------------------------------

Toolchain version selection is usually a distro configuration setting,
but you can also set this in your build/conf/local.conf file. To use
gcc 7 instead of gcc 8, set:

GCCVERSION = "7.%"

but you will also need the gcc 7 toolchain recipes in one of your layers,
since it was retired from OE-Core in favor of gcc 8.


Contributing
------------

Please use GitHub (https://github.com/madisongh/meta-tegra) to submit
issues or pull requests, or add to the documentation on the wiki.
Contributions are welcome!
